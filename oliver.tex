\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{a4wide}
\usepackage{fancyhdr}
\usepackage{float}


\pagestyle{fancy}
\fancyhf{}
\lhead{MXB102 Test Notes}
\rhead{Oliver Strong n11037580}
\cfoot{Page \thepage}

\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\I}{\mathbb{I}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}

\newcommand{\contradiction}{
    {\hbox{
        \setbox0=\hbox{$\mkern-3mu{\times}\mkern-3mu$}
        \setbox1=\hbox to0pt{\hss\copy0\hss}
        \copy0\raisebox{0.5\wd0}{\copy1}\raisebox{-0.5\wd0}{\box1}\box0}}
}

\begin{document}
\section{Week 1}
All logic symbols
\begin{table}[H]
    \centering
    \begin{tabular}{ c|c|c|c|c|c|c } 
        OR & AND & NOT & Conditional (Implies) & Iff & For all & There exists\\
        \hline
        \(\lor\) & \(\land\) & \(\neg\) & \(\implies\) & \(\iff\) & \(\forall\) & \(\exists\)\\
    \end{tabular}
\end{table}
Tautology means a statement that is always true. A statement that is always false is called a paradox.
The converse of the statement "if P then Q" (\(P \implies Q\)) is the statement "if Q then P" (\(Q \implies P\)).
\section{Week 2}
Set notation:
\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    Empty set & Is in & not in & strict subset & subset & set union & set intersection & set difference\\
    \hline
    \(\emptyset\) & \(\in\) & \(\notin\) & \(\subset\) & \(\subseteq\) & \(\cup\) & \(\cap\) & \(\setminus\)
    \end{tabular}
\end{table}
The set difference is read from left to right. \(S\setminus T\) Is in S but not in T.

Let "0" = {}, the empty set.

Successor function: \(S(n) = n \cup {n}\)

Addition (+) is defined recursively by the following rules/axioms:

(A1) "a" + "0" = "a"

(A2) "a" + S("b") = S("a" + "b")

Multiplication (.) is also defined recursively by the following rules/axioms:

(M1) "a" . "0" = "0"

(M2) "a" . S("b") = "a" + ("a" . "b")

These also apply

Commutativity of addition: "a" + "b" = "b" + "a"

Associativity of addition: ("a" + "b") + "c" = "a" + ("b" + "c")

Commutativity of multiplication: "a" . "b" = "b" . "a"

Associativity of multiplication: ("a" . "b") . "c" = "a" . ("b" . "c")

Distributivity of multiplication over addition: "a" . ("b" + "c") = ("a" . "b") + ("a" . "c")

Cancellation law: If "a" . "b" = "a" . "c" and "a" \(\neq\) "0" then "b" = "c"
\section{Week 3}
Definition: the cartesian product (\(\times\))
The cartesian product of two sets S and T is the set \(A \times T\) consisting of all ordered pairs (x,y) where \(x \in S\) and \(y \in T\)

Definition: binary relation
Let S be a set. A binary relation R on S is the subset \(S\times S\). If, a, b \(\in\) S and (a,b) \(\in\) R, we write aRb and say "a is related to b under R"

Relation properties:
\begin{enumerate}
    \item reflexive:
    \(\forall x \in S: xRx\)
    \item symmetric: 
    \(\forall x,y \in S: if xRy then yRx\)
    \item antisymmetric: 
    \(\forall \text{distinct} x,y \in S\), if xRy then NOT(yRx)
    \item transitive:
    \(\forall x,y,z \in S\): if xRy and yRz, then xRz
\end{enumerate}

Definition: Equivalence relation
An equivalence relation is a relation which is reflexive, symmetric, and transitive

Definition: function (the rule version)
Let S and T be sets. A function \textit{f} from S to T, denoted \(f:S \to T\), is a rule that assigns to each element \(x \in S\) a unique element \(f(x) \in T\)

S is called the domain and T is called the codomain. The set \(f(x) = {f(x): x\in S}\) is called the image of \textit{f}.

Definition: function (the relation version)
A function \textit{f} from S to T is a relation R between S and T that is 
\begin{enumerate}
    \item functional:
    \(\forall x \in S \text{\ and\ } \forall y,z \in T:\) if xRy and xRz, then y=z
    \item left-total: 
    \(\forall x \in S: \exists y \in T: xRy\)

\end{enumerate}
Function properties:
\begin{enumerate}
    \item \textbf{injective} or \textbf{one-to-one}:
    \(\forall x,z \in S\): \(\forall y \in T\): if xRy and zRy then x=z
    \item \textbf{surjective} or \textbf{onto}:
    \(\forall y \in T\): \(\exists x \in S\): xRy
    \item bijective:
    R is injective and surjective
\end{enumerate}

Definition: Equal functions
Two functions \textit{f} and \textit{g} are \textbf{equal} if they have the same domains, the same codomains, and for all x in the domain \(f(x) = g(X)\)

Equivalently, they are equal if they are the same subset of domain \(\times\) codomain


Definition: Composition
Let \(f: X \to Y\) and \(g: Y \to Z\). The composition of \textit{f} and \textit{g}, \(g \circ f\), maps X to Z and is defined as 
\begin{equation}
\begin{split}
    (g \circ f)(x) &= g(f(x))\\
    h \circ (g \circ f) &= (h \circ g) \circ f
\end{split}
\end{equation}

Definition: Identity function

The identity function of X is the function \(1_x:X \to X\) defined by \(1_x(x) =x\) for all \(x \in X\)

Definition: Inverse functions

If \(f: X \to Y\) and \(g: Y\to X\) are function such that \(g \circ f = 1_x\) and \(f \circ g = 1_y\), then g is called the inverse of \textit{f}, i.e. \(g=f^{-1}\)

A function has an inverse iff the function is bijective.
\begin{equation}
    f \implies f^{-1} \iff f \text{\ is bijective}
\end{equation}

\section{Week 4}
Definition: Equivalence class

Let R be an equivalence relation on S. Let \(x \in S\). The \textbf{equivalence class} of x, denoted [x], is the set of all things in S that are related to x:
\begin{equation}
    [x] = {y \in S: xRy}
\end{equation}
x is called a \textbf{representative} of the equivalence class [x]
Example: Since R is an equivalence relation (reflexive, symmetric, and transitive) then \(x \in [x]\)

Definition: Rational numbers

Define the equivalence relation {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
 on \(\Z \times (\N\setminus {0})\) by \((a,b) {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
 (c,d) \iff ad=bc\).

The equivalence classes of {\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
 are called rational numbers. We represent [(a,b)] as a/b or \(\frac{a}{b}\).

The set of all rational numbers is 
\begin{equation}
    \Q = {a/b: a \in Z, b \in \N, b \neq 0}
\end{equation}

Addition of Rational numbers:
(a,b) + (c,d) = (ad+bc, bd)

Multiplication of Rational numbers
(a,b) \(\times\) (c,d) = (ac,bd)
\newpage
Definition: \textbf{Commutative ring with identity}

A \textbf{commutative ring with identity} is a set S together with two binary operations  + and \(\cdot\) such that 
\begin{itemize}
    \item (C1) + is closed: \(\forall a,b \in S, a+b \in S\)
    \item (A1) + is associative: \(\forall a,b,c \in S, (a+b)+c = a+(b+c)\)
    \item (A2) + is commutative: \(\forall a,b \in S, a+b=b+a\)
    \item (A3) additive identity: \(\exists\) an element \(z \in S\) such that \(a+z=a\) for all \(a\in S\)
    \item (A4) additive inverse: \(\forall a \ in S, \exists b\in S: a+b = z\)
    \item (C2) \(\cdot\) is closed: \(\forall a,b \in S, a.\cdot b \in S\)
    \item (M1) \(\cdot\) is associative: \(\forall a,b,c \in S\), \(a \cdot (b \cdot c) = (a \cdot c) \cdot c\)
    \item (M2) \(\cdot\) is commutative: \(\forall a,b \in S\), \(a \cdot b = b \cdot a\)
    \item (M3) multiplicative identity: \(\forall a \in S, \exists e \in S: a \cdot e = a\) (Often denoted "1")
    \item (D) distributivity: \(\forall a,b,c \in S, a \cdot (b + c) = (a \cdot b) + (a \cdot c)\)
\end{itemize}

Definition: \textbf{Integers}

We define the set of \textbf{integers} to be the smallest set that contains the natural numbers and is a ring. \(\N\) fails (A4) so it is not a \textbf{commutative ring with identity}. \(\Z\) is the closure of \(\N\) with respect to the ring axioms.

Definition: Field

Let \(S\) be a ring with at least one non-zero element. S is a \textbf{field} if it also satisfies:
\begin{itemize}
    \item (M4) \textbf{multiplicative inverse}: \(\forall a \in S\setminus {0}, \exists b \in S: a \cdot b = e\)
\end{itemize}

Things like \(\R, \C, \text{ and } \Q\) satisfy the definition of a field.

\vspace{\baselineskip}
Constructing the \textbf{real numbers} (as \(\infty\) decimals)

Define the set \textit{D} as the cartesian product of the integers \(\Z\) and the infite sequence of digits {0,1,2,3,4,5,6,7,8,9}

\(a\in D\) consists of an integer \(a_0\) and a sequence of digits \(\{a_n\}^{\infty}_{n=1}\)

\vspace{\baselineskip}
Constructing the \textbf{real numbers} (equivalence relation)

Equivalence relation defines that the following two representations are equal
\begin{equation}
    x_0.x_1x_2...x_{k-1}x_k\overline{000} = x_0.x_1x_2...x_{k-1}(x_k-1)\overline{999}
\end{equation}
where \(x_k \neq 0\)

\vspace{\baselineskip}
Definition \textbf{Real numbers} (infinite decimal version)

Define the set D as the cartesian product of the integers \(\Z\) and the infinite sequence of digits {0,1,2,3,4,5,6,7,8,9}.

The set of \textbf{real numbers} denoted by \(\R\), is the set of equivalence classes of D using the equivalence relation
\begin{equation}
    x_0.x_1x_2...x_{k-1}x_k\overline{000} = x_0.x_1x_2...x_{k-1}(x_k-1)\overline{999}
\end{equation}
where \(x_k \neq 0\)

\vspace{\baselineskip}
Definition \textbf{Irrational numbers}
The irrational numbers sometimes denoted by \(\I\) or \(\overline{\Q}\), is the set of all real numbers that are not rational numbers. \(\I = \R \setminus \Q\)

\vspace{\baselineskip}
Definition: \textbf{Cardinality}

If there exists a bijection from the set \textit{X} to the set \textit{Y}, we write \(\#X = \#Y\) or \(|X| = |Y|\), and we say that \textit{X} and \textit{Y} have the same \textbf{cardinality}. For example \(A = \Z, B = \text{ even }\Z\) the function between them with a bijection is \(f(x) = 2x\) which maps from \(A\to B\).

\vspace{\baselineskip}
Definition: \(\N_{<n}\)

Let \(\N_{<n}\) denote the subset of \(\N\) consisting of all natural numbers less than \textit{n}

\vspace{\baselineskip}
Definition: \textbf{Finite/infinite set}

Let X be a set. If \(\#X = \#\N_{<n}\), then we say that the \textbf{number of elements} of \textit{X} is \textit{n}, and \textit{X} is called a \textbf{finite set}.

If there is no \(n \in \N\) such that there is a bijection beween \textit{X} and \(\N_{<n}\), then \textit{X} is called an \textbf{infinite set}.

\vspace{\baselineskip}
Definition: \textbf{Countably/uncountably infinite}

If \textit{X} is an infinite set and there exists a bijection between \textit{X} and \(\N\), then we say that \textit{X} is \textbf{countably infinite}, otherwise we can say the \textit{X} is \textbf{uncountably infinite}

\vspace{\baselineskip}
\textbf{\underline{Theorem}} \(\#\N = \#\Z\)

There exists a bijection between \(\Z\) and \(\N\).
\begin{equation}
    f(x) = 
    \begin{cases} 
       -k & \text{if} x=2k \\
       k+1 & \text{if} x=2k+1
   \end{cases}    
\end{equation}
The inverse function can then be used to prove bijectivity.

\vspace{\baselineskip}
\textbf{\underline{Theorem}} \(\#\N = \#\Q\)

\section{Week 5}

Proof by contradiction:
\begin{enumerate}
    \item Assume the statement is false
    \item Make a sequence of logical statements that follow from that assumption
    \item Arrive at a contradiction
    \item Since the steps arrived at something false, the original assumption must have been wrong
    \item Thus the statement is true
\end{enumerate}
A \(\contradiction\) symbol is used to show that we have arrived at a contradiction. Then conclude and remember the QED or \(\square\)

\section{Week 6}
Definition: \textbf{Contrapositive}

The \textbf{contrapositive} of the statement 
\begin{equation}
    P \implies Q
\end{equation}
is the statement
\begin{equation}
    (NOT(Q)) \implies (NOT(P))
\end{equation}
These are equivalent statements

\newpage
\textbf{Proof Method: by contrapositive}
\begin{itemize}
    \item Write the original statement in the form:
    
    "Let \(x,y\) be ... if \(P\), then \(Q\)"
    \item Rewrite the statement in the contrapositive form:
    
    "Let \(x,y\) be ... if (NOT(\textit{Q})), then (NOT(\textit{P}))"
    \item Try to prove the contrapositve statement, perhaps using a direct proof
    \item Once you have proven the contrapositive, you have also proven the original statement.
\end{itemize}

\vspace{\baselineskip}
\textbf{Theorem: Principle of mathematical induction}

Let \(P(n)\) be a statement about a non-zero natural number \textit{n}.

\hspace{\parindent}\textbf{IF}

\hspace{\parindent} 1. P(1) is true,

\hspace{\parindent} \textbf{AND}

\hspace{\parindent} 2. whenever P(k) is true for \(k \geq 1\), then \(P(k+1)\) is also true,

\hspace{\parindent} \textbf{THEN}

\hspace{\parindent} \(P(n)\) is true for all non-zero natural numbers \(n\)

\textbf{Proof Method: By induction}
\begin{itemize}
    \item Figure out what the logical statement \(P(n)\) is so that the theorem can be written in the form:
    
    \textbf{"Show \(P(n)\) is true for all \(n \in \N \setminus {0}\)"}
    \item \textbf{Prove} that the \textbf{base case is true}, namely that \textbf{\(P(n)\) is true when \textit{n} = 1}
    \item Write out the \textbf{inductive hypothesis}:
    
    "Assume \(P(n)\) is true for\(n=k,k \geq 1\), namely that ... (something involving k)..."
    \item \textbf{Prove the inductive step is true}, usually by a direct proof. (Remember to make use of the inductive hypothesis)
    \item \textbf{Conclusion}: Since \(P(1)\) is true, and since \(P(k+1)\) is true whenever \(P(k)\) is true, the principle of mathematical induction implies \(P(n)\) is true for all \(n \in \N \setminus {0}\)
\end{itemize}

\section{Week 7}
\textbf{Theorem: Principle of strong induction}

Let \(P(n)\) be a statement about a non-zero natural number n.

\hspace{\parindent} \textbf{IF}

\hspace{\parindent} 1. P(1) is true,

\hspace{\parindent} \textbf{AND}

\hspace{\parindent} 2. Whenever P(1), P(2), ... P(k) are all true, then P(k+1) is also true,\(\forall k \geq 1\)

\hspace{\parindent} \textbf{THEN}

\hspace{\parindent} P(n) is true for all non-zero natural numbers n

It is essentially the same as proof by induction but it has more than one initial condition that needs to be shown to be true.

\vspace{\baselineskip}
\textbf{Definition: Binomial coefficient}

If \(0 \leq k \leq n\), then the \textbf{binomial coefficient} \(\binom{n}{k}\) pronounced "\textit{n \textbf{choose} k}", is defined
\begin{equation}
    \binom{n}{k} = \frac{n!}{k!(n-k)!}
\end{equation}

\section{Week 8}
\textbf{Definition: Geometric sequence}

Geometric sequence has the general term as \(ar^{n-1}\) where \textit{a} is the first term and \textit{r} is the ratio.
\begin{equation}
    a,ar,ar^2,ar^3
\end{equation}

\vspace{\baselineskip}
\textbf{Definition: Infinite sequence}

An \textbf{infinite sequence} on \textit{X} is a function with a domain \(\N_{>0}\) and codomain \textit{X}. That is
\begin{equation}
    a:\N_{>0} \to X
\end{equation}

We often write \(a_n\) instead of \(a(n)\), and the sequence can be represented as
\begin{equation}
    \{a_n\}^{\infty}_{n=1}
\end{equation}

\vspace{\baselineskip}
\textbf{Definition: Convergence of an infinite sequence.}

A sequence on \(\R\) is said to \textbf{converge} to \(a \in \R\) if for all \(\epsilon > 0\) there exists \(n_0 \in \N_{>0}\) such
\begin{equation}
    a - \epsilon < a_n < a + \epsilon
\end{equation}
for every \(n \geq n_0\). In other words,
\begin{equation}
    \forall \epsilon > 0: \exists n_0 \in \N_{>0}: \forall n \geq n_0: a-\epsilon<a_n<a+\epsilon
\end{equation}
We write
\begin{equation}
\lim_{n \to \infty}a_n = a\text{, or }a_n \to a\text{, as }n \to \infty\text{, or }\lim a_n = a
\end{equation}

\textbf{Definition: Divergence of an infinite sequence}
A sequence that does not converge to some finite limit is said to \textbf{diverge}

We say \(\lim_{n \to \infty} = \infty\) if for all \(N \in \R\) there exists \(n_0 \in \N_{>0}\) such that \(a_n > N\) for every \(n\geq n_0\)
\begin{equation}
    \forall N \in \R:\exists n_0 \in \N_{>0}:\forall n \geq n_0: a_n > N
\end{equation}
Thus, eventually the sequence \(\{a_n\}^{\infty}_{n=1}\) becomes arbitrary large, that is larger than any real number. (Approaching \(\infty\))

We say \(\lim_{n\to \infty}a_n = -\infty\) if for all \(M \in \R\) there exists \(n_0 \in \N_{>0}\) such that \(a_n < M\) for every \(n \geq n_0\). In other words,
\begin{equation}
    \forall M \in \R:\exists n_0 \in \N_{>0}:\forall n \geq n_0: a_n < M
\end{equation}
Thus, eventually the sequence \(\{a_n\}^{\infty}_{n=1}\) becomes smaller than any real number (approaches \(-\infty\)).

\vspace{\baselineskip}
\textbf{Theorem} Supposed that \(\{a_n\}^{\infty}_{n=1}\) and \(\{b_n\}^{\infty}_{n=1}\) are convergent infinite sequences on \(\R\) that converge to \textit{a} and \textit{b} respectively, and let \(c \in \R\) be a constant. then
\begin{itemize}
    \item \(\lim_{n\to \infty}c=c\)
    \item \(\lim_{n \to \infty}ca_n = ca\)
    \item \(\lim_{n \to \infty}(a_n + b_n) = a+b\)
    \item \(\lim_{n \to \infty}(a_n - b_n) = a-b\)
    \item \(\lim_{n \to \infty}(a_nb_n) = ab\)
    \item \(\lim_{n \to \infty}\frac{a_n}{b_n} = \frac{a}{b}\) if \(b \neq 0\)
\end{itemize}

\textbf{Theorem} A sequence \(\{a_n\}^{\infty}_{n=1}\) converges to a if and only if sequence \(\{a_2n\}^{\infty}_{n=1}\) (the even numbered terms) and the sequence \(\{a_2n-1\}^{\infty}_{n=1}\) (the odd numbered terms) both converge to \textit{a}.

\vspace{\baselineskip}
\textbf{Theorem 6.11. Squeeze theorem for sequences}

Let \(\{a_n\}^{\infty}_{n=1}\), \(\{b_n\}^{\infty}_{n=1}\), and \(\{c_n\}^{\infty}_{n=1}\)  be infinite sequences such that \(a_n \leq b_n \leq c_n\) for all \(n>n_0\). If \(\lim_{n\to\infty}a_n = \lim_{n\to\infty}c_n =d\), then \(\lim_{n\to\infty}b_n=d\)

\vspace{\baselineskip}
An infinite sequence can also be defined \textbf{\underline{recursively}} via \(a_1 = a\in\R\) and \(a_{n+1} = f(a_n)\), for \(n > 1\). We call \(f\) the \textbf{\underline{recursive formula}}

\textbf{Example}
\begin{equation}
    a_1=1, a_{n+1} = \frac{1}{2}\left(a_n + \frac{9}{a_n}\right)\text{, for }n>1
\end{equation}
\textbf{In fact, this relation can be used to approximate \(\sqrt{c}\)}
\begin{equation}
    c_1 = c_0, c_{n+1} = \frac{1}{2}\left(c_n+\frac{c}{c_n}\right)\text{, for }n>1
\end{equation}

\vspace{\baselineskip}
\textbf{Theorem}

Let \(f(x):[1,\infty)\to \R\) such that \(\lim_{x\to\infty}f(x)=L\)

then for \(n \in \N_{>0}\) we have that \(\lim_{n\to\infty}f(x)=L\)

An infinte sequence \(a_n\) on \(\R\) is defined as a function \textit{a} with the domain \(\N_{>0}\) and codomain \(\R\). However, sometimes we can also consider \(a\) as a "regular" function from \([1,\infty]\) to \(\R\). That is,
\begin{equation}
    a(x):[1,\infty]\to\R
\end{equation}
This can be interpreted as a generalisation of the infinite sequence \(a_n\).

\section{Week 9}
\textbf{Definition: Cauchy Sequence}

A sequence \(\{a_n\}^{\infty}_{n=1}\}\) on \(\R\) is said to be a \textbf{Cauchy Sequence} if for all \(\epsilon > 0\) there exists \(n_0 \in \N_{>0}\) such that \(|a_n - a_m| < \epsilon\) for every \(n,m \geq n_0\).
In other words,
\begin{equation}
    \forall \epsilon > 0: \exists n_0 \in \N_{>0}: \forall n,m \geq n_0 : |a_n-a_m|<\epsilon
\end{equation}

\vspace{\baselineskip}
\textbf{Theorem}: A sequence on \(\R\) is convergent if and only if it is a Cauchy Sequence
\begin{itemize}
    \item Proof falls outside the scope of this unit
    \item Since all Cauchy sequences converge on \(\R\) we say that \(\R\) is \textbf{complete}
    \item \(\Q\) is not complete (that is, we can create a Cauchy sequence on \(\Q\) that does not converge to an element in \(\Q\), see workshop)
\end{itemize}
    
\vspace{\baselineskip}
\textbf{Definition: Monotone Sequences}

We call a sequence \(\{a_n\}^{\infty}_{n=1}\)
\begin{itemize}
    \item \textbf{Strictly increasing} if \(a_n < a_{n+1}\) for all \textit{n}
    \item \textbf{Increasing} if \(a_n \leq a_{n+1}\) for all \textit{n}
    \item \textbf{Strictly increasing} if \(a_n > a_{n+1}\) for all \textit{n}
    \item \textbf{Decreasing} if \(a_n \geq a_{n+1}\) for all \textit{n}
\end{itemize}

We call a sequence \textbf{(strictly) monotone} if it is either (strictly) increasing or (strictly) decreasing.

\vspace{\baselineskip}
\textbf{Definition:}

If an infinite sequence has a certain property after discarding finitely many terms from the beginning of the sequence, then we say that the infinite sequence has that property \textbf{eventually}

\vspace{\baselineskip}
\textbf{Definition: Bounded}

An infinite sequence is bounded if there exists a \textit{K} and \textit{L} such that \(L \leq a_n \leq K\) for all \textit{n}.

\textbf{Theorem:} Every bounded eventually monotone sequence on \(\R\) converges.

\vspace{\baselineskip}
\textbf{Theorem: Difference test}

A sequence \(\{a_n\}^{\infty}_{n=1}\) is
\begin{itemize}
    \item strictly increasing if \(a_{n+1} - a_n > 0\) for all \textit{n}
    \item increasing if \(a_{n+1} - a_n \geq 0\) for all \textit{n}
    \item strictly decreasing if \(a_{n+1} - a_n < 0\) for all \textit{n}
    \item decreasing if \(a_{n+1} - a_n \leq 0\) for all \textit{n}
\end{itemize}

\vspace{\baselineskip}
\textbf{Theorem: Ratio test}

A sequence \(\{a_n\}^{\infty}_{n=1}\) for which all \(a_n\) are positive is
\begin{itemize}
    \item strictly increasing if \(a_{n+1} / a_n > 1\) for all \textit{n}
    \item increasing if \(a_{n+1} / a_n \geq 1\) for all \textit{n}
    \item strictly decreasing if \(a_{n+1} / a_n < 1\) for all \textit{n}
    \item decreasing if \(a_{n+1} / a_n \leq 1\) for all \textit{n}
\end{itemize}
Furthermore, if \(\lim_{n\to\infty}a_{n+1}/a_n = L < 1\), then \(a_n \to 0\) as \(n \to \infty\).

\vspace{\baselineskip}
\textbf{Theorem: Derivative test}

If  \(\{a_n\}^{\infty}_{n=1}\) is an infinite sequence on \(\R\) and if \(a(x)\) is differentiable for \(x \in \R_{\geq 1}\) then \(\{a_n\}^{\infty}_{n=1}\)
\begin{itemize}
    \item strictly increasing if \(f'(x) > 0\) for all \(x\in\R_{\geq 1}\)
    \item increasing if \(f'(x) \geq 0\) for all \(x\in\R_{\geq 1}\)
    \item strictly decreasing if \(f'(x) < 0\) for all \(x\in\R_{\geq 1}\)
    \item decreasing if \(f'(x) \leq 0\) for all \(x\in\R_{\geq 1}\)
\end{itemize}

\section{Week 10}

\textbf{Example: (Series)}

When we write \(\frac{1}{9} = 0.\overline{11111}\) we mean:
\begin{equation}
    \frac{1}{9} = 0.1 + 0.01 + 0.001 + 0.0001 + ... = \sum^{\infty}_{i=1}\frac{1}{10^i}
\end{equation}

\textbf{Definition: Infinite series}

An \textbf{infinite series} is an expression that can be written as 
\begin{equation}
    \sum^{\infty}_{i=1}a_i = a_1 + a_2 + a_3 + ...
\end{equation}

\textbf{Definition: sequence of partial sums}

Given a sequence \(\{a_n\}^{\infty}_{n=1}\) we can consider its associated \textbf{sequence of partial sums} \(\{s_n\}^{\infty}_{n=1}\) defined by
\begin{equation}
    s_n = \sum^{n}_{i=1}a_i
\end{equation}
We call \(s_n\) the \textbf{nth partial sum} of the infinite series \(\sum^{\infty}_{i=1}a_i\)

\vspace{\baselineskip}
\textbf{Definition: convergence of series}

the infinite series \(\sum^{\infty}_{i=1}a_i\) is said to \textbf{converge} to \textit{A} if the associated sequence of partial sums \(\{s_n\}^{\infty}_{n=1}\) with \(s_n = \sum^{n}_{i=1}a_i\) converges to \textit{A}. We write
\begin{equation}
    \sum^{\infty}_{i=1}a_i = A
\end{equation}
If the sequence of partial sums diverges, then the series is said to \textbf{diverge}. If \(\lim_{n\to\infty}s_n = \pm \infty\) then we also write \(\sum^{\infty}_{i=1}a_i = \pm\infty\)

\vspace{\baselineskip}
\textbf{Theorem:}

If \(\sum^{\infty}_{i=1}a_i\) converges then \(\lim_{i\to\infty}a_i = 0\)

\vspace{\baselineskip}
\textbf{Theorem (Divergence test):}
If \(\lim_{i\to\infty}a_i \neq 0\) then \(\sum^{\infty}_{i=1}a_i\) diverges

\vspace{\baselineskip}
\textbf{Geometric series:}
\begin{equation}
    \sum^{\infty}_{i=0}ar^i = a + ar + ar^2 + ... (a \neq 0)
\end{equation}
Note: the sum has to start aat 0.
\(s_0\) is the \textbf{zeroth partial sum}

\vspace{\baselineskip}
\textbf{Theorem}

A geometric series \(\sum^{\infty}_{i=0}ar^i\)(with \(a\neq 0\)) converges to \(\frac{a}{1-r}\) if \(|r|<1\) and diverges otherwise

\vspace{\baselineskip}
\textbf{Definition: Harmonic Series}

\begin{equation}
    \sum^{\infty}_{i=1}\frac{1}{i}
\end{equation}
This is a divergent series even though \(\lim_{n\to\infty}\frac{1}{n}=0\)

\vspace{\baselineskip}
\textbf{Example: telescoping series} \(\sum^{\infty}_{i=1}\frac{1}{i(i+1)}\)

The general form for the nth partial sum is \(S_n = 1-\frac{1}{n+1}\)

\vspace{\baselineskip}
\textbf{Theorem:}
\begin{enumerate}
    \item Suppose the \(\sum^{\infty}_{i=1}a_i\) and \(\sum^{\infty}_{i=1}b_i\) are two convergent infinite series. Then
    \begin{enumerate}
        \item \(\sum^{\infty}_{i=1}(a_i+a_i)\) is a convergent infinite series and \(\sum^{\infty}_{i=1}a_i+b_i\) = \(\sum^{\infty}_{i=1}a_i\) + \(\sum^{\infty}_{i=1}b_i\)
        \item \(\sum^{\infty}_{i=1}a_i-b_i\) is a convergent infinite series and \(\sum^{\infty}_{i=1}a_i-b_i\) = \(\sum^{\infty}_{i=1}a_i\) - \(\sum^{\infty}_{i=1}b_i\)
    \end{enumerate}
    \item Let \(\sum^{\infty}_{i=1}c_i\) be an infinite series and let \(d\in\R\) be a constant. Then \(\sum^{\infty}_{i=1}c_i\) and \(\sum^{\infty}_{i=1}d_i\) both converge or diverge. If they converge, we have \(\sum^{\infty}_{i=1}dc_i\) = \(d\sum^{\infty}_{i=1}c_i\)
    \item The infinite series \(\sum^{\infty}_{i=1}e_i\) and the infinite series \(\sum^{\infty}_{i=k}e_i\), with \(K \in \N\) both converge or both diverge. That is, the convergence/divergence of an infinite series is not affected by deleting (or adding) finitely many terms. The actual sum is of course affected.
    \item Suppose that \(\sum^{\infty}_{i=1}f_i\) is a divergent infinite series and \(\sum^{\infty}_{i=1}g_i\) is a convergent infinite series. Then \(\sum^{\infty}_{i=1}(f_i + g_i)\) and \(\sum^{\infty}_{i=1}(f_i - g_i)\) are both divergent.
    \item Suppose that \(\sum^{\infty}_{i=1}a_i\) and \(\sum^{\infty}_{i=1}b_i\) are both divergent infinite series then \(\sum^{\infty}_{i=1}(a_i+b_i)\) and \(\sum^{\infty}_{i=1}(a_i-b_i)\) can be either convergent or divergent.
\end{enumerate}

\section{Week 11}
\textbf{Theorem: Integral Test}
Let \(\sum^{\infty}_{i=1}a_i\) be an infinite series with only positive terms. If there exists a function \textit{f} on \([a,\infty)\) that is decreasing, continuous and such that \(a_k = f(k)\) for all \(k \geq a\), then \(\sum^{\infty}_{i=1}a_i\) and \(\int^{\infty}_{x=a}f(x)dx\) both converge or both diverge and in fact 
\begin{equation}
    \sum^{\infty}_{i=2}a_i < \int^{\infty}_{1}f(x)dx < \sum^{\infty}_{i=1}a_i
\end{equation}

\textbf{Theorem: p-series or hyperharmonic series}

Let \(p \in \R_{>0}\). Then, the p-series
\begin{equation}
    \sum^{\infty}_{i=1}\frac{1}{i^p}
\end{equation}
Converges if and only if \(p > 1\).

\vspace{\baselineskip}
\textbf{Theorem: Comparison Test}

Let \(\sum^{\infty}_{i=1}a_i\) and \(\sum^{\infty}_{i=1}b_i\) be two infinite series with non-negative terms such that \(a_k \leq b_k\), for all \(k \in \N_{>0}\). Then
\begin{itemize}
    \item If \(\sum^{\infty}_{i=1}b_i\) converges then \(\sum^{\infty}_{i=1}a_i\) also converges; and 
    \item If \(\sum^{\infty}_{i=1}a_i\) diverges then \(\sum^{\infty}_{i=1}b_i\) also diverges
\end{itemize}

\vspace{\baselineskip}
\textbf{Theorem: Limit Comparison Test}

Let \(\sum^{\infty}_{i=1}a_i\) and \(\sum^{\infty}_{i=1}b_i\) be two infinite series with positive terms such that \(\lim_{i\to\infty}a_i/b_i = c\). If \(c > 0\) and finite then both series either converge or diverge.

\vspace{\baselineskip}
\textbf{Theorem: Ratio Test for Series}

Let \(\sum^{\infty}_{i=1}a_i\) be an infinite series with positive terms such that \(\lim_{i\to\infty}a_{i+1}/a_i = c\).

Then
\begin{itemize}
    \item If \(0 \leq c < 1\), then the series converges;
    \item If \(c > 1\) (Including +\(\infty\)) then the series diverges; and
    \item If \(c = 1\) the test in inconclusive
\end{itemize}

\vspace{\baselineskip}
\textbf{Theorem: Root Test}

Let \(\sum^{\infty}_{i=1}a_i\) be an infinite series with positive terms such that \(\lim_{i\to\infty}(a_i)^{1/i}=c\).

Then
\begin{itemize}
    \item If \(0 \leq c < 1\), then the series converges;
    \item If \(c > 1\) (Including +\(\infty\)) then the series diverges; and
    \item If \(c = 1\) the test in inconclusive
\end{itemize}

\vspace{\baselineskip}
\textbf{Definition: Alternating Series}

An \textbf{alternating series} is a series of the form
\begin{equation}
    \begin{split}
        &\sum^{\infty}_{i=1}(-1)^ia_i\\
        &\text{or}\\
        &\sum^{\infty}_{i=1}(-1)^{i+1}a_i\\
    \end{split}
\end{equation}

\textbf{Theorem: Alternating Series Test}

An alternating series converges if
\begin{itemize}
    \item \(a_i \geq a_{i+1}\text{ for all }\N_{>0}\); and
    \item \(\lim_{i\to\infty}a_i = 0\)
\end{itemize}

\vspace{\baselineskip}
\textbf{Definition: Absolute convergence}

An infinite series \(\sum^{\infty}_{i=1}a_i\) is said to \textbf{converge absolutely} is the series of absolute values \(\sum^{\infty}_{i=1}|a_i|\) converges and the series is said to \textbf{diverge absolutely} is the series of absolute values \(\sum^{\infty}_{i=1}|a_i|\) diverges

\vspace{\baselineskip}
\textbf{Definition: Conditional convergence}

A convergent infinite series that diverges absolutely is said to \textbf{converge conditionally}

\vspace{\baselineskip}
\textbf{Theorem}

If an infinite series is absolute convergent then it is also convergent.

\vspace{\baselineskip}
\textbf{Theorem: Ratio Test for Absolute Convergence}

Let \(\sum^{\infty}_{i=1}a_i\) be an infinite series with nonzero terms such that
\begin{equation}
    \lim_{i\to\infty}\frac{|a_{i+1}|}{|a_i|} = c
\end{equation}

Then: 
\begin{itemize}
    \item If \(c < 1\) then the series converges absolutely and hence it also converges
    \item If \(c > 1\) (including \(+\infty\)) then the series diverges
    \item If \(c = 1\) the test is inconclusive
\end{itemize}

\section{Week 12}
\textbf{Definition: Function (rule version).}

Let \textit{S} and \textit{T} be sets. A \textbf{function} \textit{f} from \textit{s} to \textit{T}, denoted \(f: S \to T\), is a rule that assigns to each element \(x \in S\) a unique element \(f(x) \in T\)

\textit{S} is called the \textbf{domain} and \textit{T} is called the \textbf{codomain}. The set \(f(S) = \{f(x): x \in S\}\) is called the \textbf{image} of \textit{f}.

\vspace{\baselineskip}
\textbf{Definition: \(\epsilon - \delta\) definition for limit.}

Let \(f(x)\) be a function defined for all \textit{x} in some open interval \textit{I} containing \(x_0 \in \R\) (with the possible exception that \(f(x)\) is not defined for \(x_0\)). Then we say that \(f(x)\) has the \textbf{limit} \(L \in \R\text{ at }x_0\) if for all \(\epsilon > 0\) there exists \(\delta > 0\) such that \(|f(x) - L| < \epsilon\) for every \(x \in I\) satisfying \(|x - x_0| < \delta\). In other words
\begin{equation}
    \forall \epsilon > 0: \exists \delta > 0: \forall x \in I: 0< |x - x_0| < \delta \implies |f(x) - L| < \epsilon
\end{equation}

We write \(\lim_{x\to x_0}f(x)=L\)

\vspace{\baselineskip}
\textbf{Definition: Limit laws.}

Let \textit{f} and \textit{g} be two functions such that \(\lim_{x\to x_0}f(x)=L_1\) and \(\lim_{x \to x_0}g(x) = L_2\) where \(L_1,L_2 \in \R\). Then
\begin{itemize}
    \item "The limit of the sum is the sum of the limits"
    \begin{equation}
        \lim_{x\to x_0}(f(x) + g(x)) = \lim_{x\to x_0}f(x) + \lim_{x \to x_0}g(x) = L_1 + L_2
    \end{equation}
    
    \item "The limit of the difference is the difference of the limits"
    \begin{equation}
        \lim_{x \to x_0}(f(x) - g(x)) = \lim_{x\to x_0}f(x) - \lim_{x\to x_0} = L_1 - L_2
    \end{equation}
    \item "The limit of the product if the product of the limits"
    \begin{equation}
        \lim_{x \to x_0}\left(f(x) \times g(x)\right) = \left(\lim_{x\to x_0}f(x)\right)\times\left(\lim_{x\to x_0}g(x)\right) = L_1 \times L_2
    \end{equation}
    \item If \(L_2 \neq 0\), then "the limit of the quotient is the quotient of the limits"
    \begin{equation}
        \lim_{x\to x_0}\left(\frac{f(x)}{g(x)}\right) = \frac{\lim_{x\to x_0}f(x)}{\lim_{x\to x_0}g(x)} = \frac{L_1}{L_2}
    \end{equation}
\end{itemize}

\vspace{\baselineskip}
\textbf{Theorem}

\(\lim_{x\to x_0}f(x)\) exists if and only if both \(\lim_{x\to x_0^{+}}f(x)\) and \(\lim_{x\to x_0^{-}}f(x)\) exist and they are equal.

\vspace{\baselineskip}
\textbf{Squeeze theorem for functions}

Consider the interval \textit{I} with \(x_0 \in I\). Let \(f,g,h:I\to\R\) be functions satisfying 
\begin{equation}
    f(x) \leq g(x) \leq h(x), \forall x \in I \setminus \{x_0\}
\end{equation}
and let
\begin{equation}
    \lim_{x\to x_0}f(x) = \lim_{x\to x_0}h(x) = L
\end{equation}
then
\begin{equation}
    \lim_{x\to x_0}g(x) = L
\end{equation}
This statement also holds true for left-hand and right-hand limits as well as limits \(x \to \pm\infty\)

\vspace{\baselineskip}
\textbf{Definition: \(\epsilon - \delta\) definition for continuity.}

A function \(f(x): I \to \R\) is said to be \textbf{continuous at} \(c \in I\) if for all \(\epsilon > 0\) there exists \(\delta > 0\) such that \(|f(x) - f(c)| < \epsilon\) for every \(x\in I\) with \(0 < |x - c| < \delta\). In other words
\begin{equation}
    \forall \epsilon > 0: \exists \delta > 0: \forall x \in I: 0 < |x-c| < \delta \implies |f(x) - f(c)| < \epsilon
\end{equation}
We say that f(x) is \textbf{continuous on \textit{I}} if \textit{f} is continuous at every point of \textit{I}.

\vspace{\baselineskip}
\textbf{Theorem}

Let \textit{f} be continuous at \textit{L} and let \(\lim_{x_c}g(x) = L\). Then
\begin{equation}
    \lim_{x\to c}(f \circ g)(x) = \lim_{x \to c}f(g(x)) = f\left(\lim_{x\to c}g(x)\right) = f(L)
\end{equation}
Note that this still holds true if \textit{c} in the above limits is replaced by \(c^{\pm}\) and \(\pm\infty\)

\vspace{\baselineskip}
\textbf{Theorem}

Let \textit{f} and \textit{g} be two functions that are continuous at \textit{c} and let \(h(x)\) be continuous at \(g(c)\). Then
\begin{enumerate}
    \item \(f(x) + g(x)\) is continuous at \textit{c}
    \item \(f(x) - g(x)\) is continuous at \textit{c}
    \item \(f(x)\times g(x)\) is continuous at \textit{c}
    \item \(\frac{f(x)}{g(x)}\) is continuous at \textit{c} as long as \(g(c) \neq 0\)
    \item \(g \circ g\) is continuous at \textit{c}
\end{enumerate}

\vspace{\baselineskip}
\textbf{Theorem: L'Hopitals rule}

Let \(f(x)\) and \(g(x)\) be differentiable functions (that is \(f,g\) are continuous and their derivatives exist) except possibly at \(x_0\), such that \(\lim_{x\to x_0}f(x)=0=\lim_{x\to x_0}g(x) or \lim_{x\to x_0}f(x) = \pm\infty\) and \(\lim_{x\to x_0}g(x) = \pm\infty\). 

Then, if \(\lim_{x\to x_0}\frac{f'(x)}{g'(x)}\) exists (or is \(\pm\infty\)) we have
\begin{equation}
    \lim_{x\to x_0}\frac{f(x)}{g(x)} = \lim_{x\to x_0}\frac{f'(x)}{g'(x)}
\end{equation}
Note that this statement also holds for left-hand and right-hand limits, as well as for limits \(x \to \pm\infty\)
\end{document}
